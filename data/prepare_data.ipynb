{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maher\\Unsupervised-anomaly-detection-on-noisy-time-series-data-for-accurate-load-forecasting\n"
     ]
    }
   ],
   "source": [
    "%cd .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data_path = \"data/inpg_dataset/csv_data/\"\n",
    "feature_name = \"conso_global\"\n",
    "\n",
    "clean_contam_split = 0.1\n",
    "train_test_split = 0.7\n",
    "window_size = 24*3\n",
    "step = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "csv_paths = glob.iglob(os.path.join(csv_data_path, \"*.csv\"))\n",
    "\n",
    "for csv_path in csv_paths:\n",
    "    try: csv_file = pd.read_csv(csv_path)\n",
    "    except: continue\n",
    "    \n",
    "    serie = csv_file[feature_name].values\n",
    "    start, end = 0, window_size\n",
    "    while end<=len(serie):\n",
    "        sliding_window = serie[start: end]\n",
    "        start += step\n",
    "        end += step\n",
    "        sliding_window = (sliding_window-sliding_window.mean())/sliding_window.std() # prevents exploding gradients\n",
    "        X.append(sliding_window)\n",
    "X = np.stack(X)\n",
    "\n",
    "N = int(clean_contam_split*X.shape[0])\n",
    "M = int(train_test_split*X.shape[0])\n",
    "\n",
    "clean_data = X[0:N, :]\n",
    "train_data = X[N:M, :] \n",
    "test_data = X[M:, :]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Injecting anomalies into both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contam_prob = 0.05\n",
    "min_nbr_anom = 5\n",
    "max_nbr_anom = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contam_data = []\n",
    "train_anom_idx = []\n",
    "for sample_idx in range(train_data.shape[0]):\n",
    "    sample = train_data[sample_idx, :].copy()\n",
    "    peak_value = max(sample.max(), abs(sample.min()))\n",
    "    is_contam = random.random()<=contam_prob\n",
    "    if is_contam:\n",
    "        nbr_anom = random.randint(min_nbr_anom, max_nbr_anom)\n",
    "        sample_anom_idx = []\n",
    "        for _ in range(nbr_anom):\n",
    "            idx = random.randint(0, len(sample)-1)\n",
    "            sample[idx] = (peak_value+random.random()*peak_value)*-1**random.randint(0, 1)\n",
    "            sample_anom_idx.append(idx)\n",
    "        train_contam_data.append(sample)\n",
    "        train_anom_idx.append(sample_anom_idx)\n",
    "    else:\n",
    "        train_contam_data.append(sample)\n",
    "        train_anom_idx.append([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contam_data = []\n",
    "test_anom_idx = [] # indices of where the anomalies are for each sequence\n",
    "for sample_idx in range(test_data.shape[0]):\n",
    "    sample = test_data[sample_idx, :].copy()\n",
    "    peak_value = max(sample.max(), abs(sample.min()))\n",
    "    is_contam = random.random()<=contam_prob\n",
    "    if is_contam:\n",
    "        sample_anom_idx = []\n",
    "        nbr_anom = random.randint(min_nbr_anom, max_nbr_anom)\n",
    "        for _ in range(nbr_anom):\n",
    "            idx = random.randint(0, len(sample)-1)\n",
    "            sample[idx] = (peak_value+random.random()*peak_value)*-1**random.randint(0, 1)\n",
    "            sample_anom_idx.append(idx)\n",
    "        test_contam_data.append(sample)\n",
    "        test_anom_idx.append(sample_anom_idx)\n",
    "    else:\n",
    "        test_contam_data.append(sample)\n",
    "        test_anom_idx.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in 'data/inpg_dataset/npy_data/' have been deleted.\n",
      "All files in 'data/IRISE_dataset/npy_data/' have been deleted.\n"
     ]
    }
   ],
   "source": [
    "def delete_files_in_directory(directory_path):\n",
    "    try:\n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "        print(f\"All files in '{directory_path}' have been deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "for path in [\n",
    "    # \"data/aemo_dataset/npy_data/\", \n",
    "    \"data/inpg_dataset/npy_data/\",\n",
    "    \"data/IRISE_dataset/npy_data/\"\n",
    "    ]:\n",
    "    delete_files_in_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_data_path = \"data/inpg_dataset/npy_data/\"\n",
    "\n",
    "for i, sample in enumerate(clean_data): # clean train data to later evalute forecasting performance\n",
    "    np.save(os.path.join(npy_data_path, \"clean\", str(i)), sample)\n",
    "\n",
    "for i, (sample, anom_idx) in enumerate(zip(train_contam_data, train_anom_idx)): # contaminated train data\n",
    "    np.save(os.path.join(npy_data_path, \"train\", \"data\", str(i)), sample)\n",
    "    np.save(os.path.join(npy_data_path, \"train\", \"gt\", str(i)), anom_idx)\n",
    "\n",
    "for i, (sample, anom_idx) in enumerate(zip(test_contam_data, test_anom_idx)):   # contaminated test data\n",
    "    np.save(os.path.join(npy_data_path, \"test\", \"data\", str(i)), sample)\n",
    "    np.save(os.path.join(npy_data_path, \"test\", \"gt\", str(i)), anom_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
