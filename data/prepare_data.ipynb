{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maher\\Unsupervised-anomaly-detection-on-noisy-time-series-data-for-accurate-load-forecasting\n"
     ]
    }
   ],
   "source": [
    "%cd .. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collecting data from AEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "for year in range(2000, 2021+1): \n",
    "    for month in ['01','02','03','04','05','06','07','08','09','10','11','12']:\n",
    "        for location in [\"NSW\", \"QLD\", \"VIC\", \"SA\", \"TAS\"]:\n",
    "            bash_command = f\"curl https://aemo.com.au/aemo/data/nem/priceanddemand/PRICE_AND_DEMAND_{year}06_NSW1.csv > ./aemo_dataset/{year}{month}{location}.csv\"\n",
    "            result = subprocess.run(bash_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            if result.returncode != 0:\n",
    "                print(\"Command failed:\")\n",
    "                print(result.stderr)\n",
    "            else:\n",
    "                print(f\"Downloaded {year}-{month}-{location}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and train/test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/aemo_csv_data/\"\n",
    "train_test_split = 0.7\n",
    "window_size = 48*5\n",
    "step = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "csv_paths = glob.iglob(os.path.join(data_path, \"*.csv\"))\n",
    "for csv_path in csv_paths:\n",
    "    try: csv_file = pd.read_csv(csv_path)\n",
    "    except: continue\n",
    "    serie = csv_file[\"TOTALDEMAND\"].values\n",
    "    start, end = 0, window_size\n",
    "    while end<=len(serie):\n",
    "        sliding_window = serie[start: end]\n",
    "        start += step\n",
    "        end += step\n",
    "        sliding_window = (sliding_window-sliding_window.mean())/sliding_window.std() # prevents exploding gradients\n",
    "        X.append(sliding_window)\n",
    "X = np.stack(X)\n",
    "\n",
    "N = int(train_test_split*X.shape[0])\n",
    "train_data = X[0:N, :] # clean data\n",
    "test_data = X[N:, :]   # for evaluating reconstruction of unseen data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding anomalies to both train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contam_prob = 0.1\n",
    "min_nbr_anom = 5\n",
    "max_nbr_anom = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contam_data = []\n",
    "train_anom_idx = []\n",
    "for sample_idx in range(train_data.shape[0]):\n",
    "    sample = train_data[sample_idx, :].copy()\n",
    "    peak_value = max(sample.max(), abs(sample.min()))\n",
    "    is_contam = random.random()<=contam_prob\n",
    "    if is_contam:\n",
    "        nbr_anom = random.randint(min_nbr_anom, max_nbr_anom)\n",
    "        sample_anom_idx = []\n",
    "        for _ in range(nbr_anom):\n",
    "            idx = random.randint(0, len(sample)-1)\n",
    "            sample[idx] = (peak_value+random.random()*peak_value)*-1**random.randint(0, 1)\n",
    "            sample_anom_idx.append(idx)\n",
    "        train_contam_data.append(sample)\n",
    "        train_anom_idx.append(sample_anom_idx)\n",
    "    else:\n",
    "        train_contam_data.append(sample)\n",
    "        train_anom_idx.append([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contam_data = []\n",
    "test_anom_idx = [] # indices of where the anomalies are for each sequence\n",
    "for sample_idx in range(test_data.shape[0]):\n",
    "    sample = test_data[sample_idx, :].copy()\n",
    "    peak_value = max(sample.max(), abs(sample.min()))\n",
    "    is_contam = random.random()<=contam_prob\n",
    "    if is_contam:\n",
    "        sample_anom_idx = []\n",
    "        nbr_anom = random.randint(min_nbr_anom, max_nbr_anom)\n",
    "        for _ in range(nbr_anom):\n",
    "            idx = random.randint(0, len(sample)-1)\n",
    "            sample[idx] = (peak_value+random.random()*peak_value)*-1**random.randint(0, 1)\n",
    "            sample_anom_idx.append(idx)\n",
    "        test_contam_data.append(sample)\n",
    "        test_anom_idx.append(sample_anom_idx)\n",
    "    else:\n",
    "        test_contam_data.append(sample)\n",
    "        test_anom_idx.append([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files in 'data/aemo_npy_data/' have been deleted.\n",
      "All files in 'data/filtered/' have been deleted.\n"
     ]
    }
   ],
   "source": [
    "def delete_files_in_directory(directory_path):\n",
    "    try:\n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                os.remove(file_path)\n",
    "        print(f\"All files in '{directory_path}' have been deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "for path in [\"data/aemo_npy_data/\", \"data/filtered/\"]:\n",
    "    delete_files_in_directory(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = \"data/aemo_npy_data\"\n",
    "\n",
    "# for i, sample in enumerate(train_data): # clean train data\n",
    "#     np.save(os.path.join(dataset_root, \"train\", \"data\", str(i)), sample)\n",
    "\n",
    "for i, (sample, anom_idx) in enumerate(zip(train_contam_data, train_anom_idx)): # contaminated train data\n",
    "    np.save(os.path.join(dataset_root, \"train\", \"data\",str(i)), sample)\n",
    "    np.save(os.path.join(dataset_root, \"train\", \"gt\", str(i)), anom_idx)\n",
    "\n",
    "for i, (sample, anom_idx) in enumerate(zip(test_contam_data, test_anom_idx)):   # contaminated test data\n",
    "    np.save(os.path.join(dataset_root, \"test\", \"data\", str(i)), sample)\n",
    "    np.save(os.path.join(dataset_root, \"test\", \"gt\", str(i)), anom_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
